{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a25678c9993c1c2bbf0167f2ff03c982",
     "grade": false,
     "grade_id": "header-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Tips\n",
    "- To avoid unpleasant surprises, I suggest you _run all cells in their order of appearance_ (__Cell__ $\\rightarrow$ __Run All__).\n",
    "\n",
    "\n",
    "- If the changes you've made to your solution don't seem to be showing up, try running __Kernel__ $\\rightarrow$ __Restart & Run All__ from the menu.\n",
    "\n",
    "\n",
    "- Before submitting your assignment, make sure everything runs as expected. First, restart the kernel (from the menu, select __Kernel__ $\\rightarrow$ __Restart__) and then **run all cells** (from the menu, select __Cell__ $\\rightarrow$ __Run All__).\n",
    "\n",
    "## Reminder\n",
    "\n",
    "- Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name, UA email, and collaborators below:\n",
    "\n",
    "\n",
    "\n",
    "Several of the cells in this notebook are **read only** to ensure instructions aren't unintentionally altered.  \n",
    "\n",
    "If you can't edit the cell, it is probably intentional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Wenmo Sun\"\n",
    "# University of Arizona email address\n",
    "EMAIL = \"wmsun@email.arizona.edu\"\n",
    "# Names of any collaborators.  Write N/A if none.\n",
    "COLLABORATORS = \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0783621da2f047c6360f2ec0d56f121c",
     "grade": false,
     "grade_id": "cell-e35b85c2416e40f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Scratchpad\n",
    "\n",
    "You are welcome to create new cells (see the __Cell__ menu) to experiment and debug your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c80ac423030cfa372644d7cd456061af",
     "grade": false,
     "grade_id": "cell-955f8133afe96b26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e07f9ac61f4be6a57b6961cde23a6d58",
     "grade": false,
     "grade_id": "cell-a2292c2fbc4cf52e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Mini Python tutorial\n",
    "\n",
    "This course uses Python 3.8.\n",
    "\n",
    "Below is a very basic (and incomplete) overview of the Python language... \n",
    "\n",
    "For those completely new to Python, [this section of the official documentation may be useful](https://docs.python.org/3.8/library/stdtypes.html#common-sequence-operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfcd9f827d4855d02514b2e54ba32077",
     "grade": false,
     "grade_id": "cell-d6593132353238c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      "[2, 3, 4, 5]\n",
      "[2]\n",
      "hello, Josuke!\n",
      "Howdy, partner!\n",
      "13\n",
      "Hi, Fred!\n",
      "[('radical', 4), ('analysis', 7), ('bighorn', 12), ('bounce', 32)]\n",
      "[('analysis', 7), ('bighorn', 12), ('bounce', 32), ('radical', 4)]\n"
     ]
    }
   ],
   "source": [
    "# This is a comment.  \n",
    "# Any line starting with # will be interpreted as a comment\n",
    "\n",
    "# this is a string assigned to a variable\n",
    "greeting = \"hello\"\n",
    "\n",
    "# If enclosed in triple quotes, strings can also be multiline:\n",
    "\n",
    "\"\"\"\n",
    "I'm a multiline\n",
    "string.\n",
    "\"\"\"\n",
    "\n",
    "# let's use a for loop to print it letter by letter\n",
    "for letter in greeting:\n",
    "    print(letter)\n",
    "    \n",
    "# Did you notice the indentation there?  Whitespace matters in Python!\n",
    "\n",
    "# here's a list of integers\n",
    "\n",
    "numbers = [1, 2, 3, 4]\n",
    "\n",
    "# let's add one to each number using a list comprehension\n",
    "# and assign the result to a variable called res\n",
    "# list comprehensions are used widely in Python (they're very Pythonic!)\n",
    "\n",
    "res = [num + 1 for num in numbers]\n",
    "\n",
    "# let's confirm that it worked\n",
    "print(res)\n",
    "\n",
    "# now let's try spicing things up using a conditional to filter out all values greater than or equal to 3...\n",
    "print([num for num in res if not num >= 3])\n",
    "\n",
    "# Python 3.7 introduced \"f-strings\" as a convenient way of formatting strings using templates\n",
    "# For example ...\n",
    "name = \"Josuke\"\n",
    "\n",
    "print(f\"{greeting}, {name}!\")\n",
    "\n",
    "# f-strings are f-ing convenient!\n",
    "\n",
    "\n",
    "# let's look at defining functions in Python..\n",
    "\n",
    "def greet(name):\n",
    "    print(f\"Howdy, {name}!\")\n",
    "\n",
    "# here's how we call it...\n",
    "\n",
    "greet(\"partner\")\n",
    "\n",
    "# let's add a description of the function...\n",
    "\n",
    "def greet(name):\n",
    "    \"\"\"\n",
    "    Prints a greeting given some name.\n",
    "    \n",
    "    :param name: the name to be addressed in the greeting\n",
    "    :type name: str\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"Howdy, {name}!\")\n",
    "    \n",
    "# I encourage you to use docstrings!\n",
    "\n",
    "# Python introduced support for optional type hints in v3.5.\n",
    "# You can read more aobut this feature here: https://docs.python.org/3.8/library/typing.html\n",
    "# let's give it a try...\n",
    "def add_six(num: int) -> int:\n",
    "    return num + 6\n",
    "\n",
    "# this should print 13\n",
    "print(add_six(7))\n",
    "\n",
    "# Python also has \"anonymous functions\" (also known as \"lambda\" functions)\n",
    "# take a look at the following code:\n",
    "\n",
    "greet_alt = lambda name: print(f\"Hi, {name}!\")\n",
    "\n",
    "greet_alt(\"Fred\")\n",
    "\n",
    "# lambda functions are often passed to other functions\n",
    "# For example, they can be used to specify how a sequence should be sorted\n",
    "# let's sort a list of pairs by their second element\n",
    "pairs = [(\"bounce\", 32), (\"bighorn\", 12), (\"radical\", 4), (\"analysis\", 7)]\n",
    "# -1 is last thing in some sequence, -2 is the second to last thing in some seq, etc.\n",
    "print(sorted(pairs, key=lambda pair: pair[-1]))\n",
    "\n",
    "# we can sort it by the first element instead\n",
    "# NOTE: python indexing is zero-based\n",
    "print(sorted(pairs, key=lambda pair: pair[0]))\n",
    "\n",
    "# You can learn more about other core data types and their methods here: \n",
    "# https://docs.python.org/3.8/library/stdtypes.html\n",
    "\n",
    "# Because of its extensive standard library, Python is often described as coming with \"batteries included\".  \n",
    "# Take a look at these \"batteries\": https://docs.python.org/3.8/library/\n",
    "\n",
    "# You now know enough to complete this homework assignment (or at least where to look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2dbde2a5d52dfc3a7056fcac987d9613",
     "grade": false,
     "grade_id": "base-imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator, Sequence, Text, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import spmatrix, vstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import itertools\n",
    "import pytest\n",
    "\n",
    "# an NDArray is either a numpy array (ndarray) or a scipy sparse matrix (spmatrix)\n",
    "NDArray  = Union[np.ndarray, spmatrix]\n",
    "# type aliases for sequences of strings\n",
    "# we'll use this type alias for our tokens\n",
    "TokenSeq = Sequence[Text]\n",
    "# ...and this one for our POS tags\n",
    "TagSeq   = Sequence[Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77dc67bfff712b9f453e67434966f28c",
     "grade": false,
     "grade_id": "random-seed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14a290184893defd5bc434e9dced5b05",
     "grade": false,
     "grade_id": "answer-imports",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Add your imports here (ex. classes from scikit-learn)\n",
    "# YOUR CODE HERE\n",
    "import re\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "664a4f9cbb35c2496fb32fac11093f18",
     "grade": false,
     "grade_id": "md-read-data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `read_ptbtagged`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "199f2e63eda7347a3a77412a9e805e15",
     "grade": false,
     "grade_id": "code-read-data",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_ptbtagged(ptbtagged_path: str) -> Iterator[Tuple[TokenSeq, TagSeq]]:\n",
    "    \"\"\"\n",
    "    Reads sentences from a Penn TreeBank .tagged file.\n",
    "    Each sentence is a sequence of tokens and part-of-speech tags.\n",
    "\n",
    "    Penn TreeBank .tagged files contain one token per line, with an empty line\n",
    "    marking the end of each sentence. Each line is composed of a token, a tab\n",
    "    character, and a part-of-speech tag. Here is an example:\n",
    "\n",
    "        What\tWP\n",
    "        's\tVBZ\n",
    "        next\tJJ\n",
    "        ?\t.\n",
    "\n",
    "        Slides\tNNS\n",
    "        to\tTO\n",
    "        illustrate\tVB\n",
    "        Shostakovich\tNNP\n",
    "        quartets\tNNS\n",
    "        ?\t.\n",
    "\n",
    "    :param ptbtagged_path: The path of a Penn TreeBank .tagged file, formatted\n",
    "    as above.\n",
    "    :return: An iterator over sentences, where each sentence is a tuple of\n",
    "    a sequence of tokens and a corresponding sequence of part-of-speech tags.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    readfile = open(ptbtagged_path).read().strip()\n",
    "#     print(readfile)\n",
    "    lines = re.sub(\"\\n\\t\\n\", \"\\n\\n\", readfile).split(\"\\n\\n\")\n",
    "#     print(lines)\n",
    "    return iter([([token_tag.split('\\t')[0] for token_tag in sent.split('\\n')], [token_tag.split('\\t')[1] for token_tag in sent.split('\\n')]) for sent in lines])\n",
    "            \n",
    "\n",
    "# read_ptbtagged('data/PTBSmall/train.tagged')      \n",
    "#     raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b9d129b5a5d6bab3cf87e2b35693811",
     "grade": false,
     "grade_id": "cell-b2bb58db7fd31606",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `Classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3480c824beb94e213b9f2e07e8bf0eb",
     "grade": false,
     "grade_id": "code-classifier",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Our MEMM\n",
    "class Classifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the classifier.\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # Use `DictVectorizer` to record your features.\n",
    "        # See https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
    "        #\n",
    "        # Minimally, you must include the following features:\n",
    "        # `token` (the current word) \n",
    "        # `pos-1` (the prior tag) \n",
    "        self.feature_encoder = DictVectorizer()\n",
    "        # multinomial logistic regression\n",
    "        self.model = LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\")\n",
    "\n",
    "#     # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "#     # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    def train(self, tagged_sentences: Iterator[Tuple[TokenSeq, TagSeq]]) -> Tuple[NDArray, NDArray]:\n",
    "        \"\"\"\n",
    "        Trains the classifier on the part-of-speech tagged sentences,\n",
    "        and returns the feature matrix and label vector on which it was trained.\n",
    "\n",
    "        The feature matrix should have one row per training token. The number\n",
    "        of columns is up to the implementation, but there must at least be 1\n",
    "        feature for each token, named \"token=T\", where \"T\" is the token string,\n",
    "        and one feature for the part-of-speech tag of the preceding token,\n",
    "        named \"pos-1=P\", where \"P\" is the part-of-speech tag string, or \"<s>\" if\n",
    "        the token was the first in the sentence. For example, if the input is:\n",
    "\n",
    "            What\tWP\n",
    "            's\tVBZ\n",
    "            next\tJJ\n",
    "            ?\t.\n",
    "\n",
    "        Then the first row in the feature matrix should have features for\n",
    "        \"token=What\" and \"pos-1=<s>\", the second row in the feature matrix\n",
    "        should have features for \"token='s\" and \"pos-1=WP\", etc. The alignment\n",
    "        between these feature names and the integer columns of the feature\n",
    "        matrix is given by the `feature_index` method below.\n",
    "\n",
    "        The label vector should have one entry per training token, and each\n",
    "        entry should be an integer. The alignment between part-of-speech tag\n",
    "        strings and the integers in the label vector is given by the\n",
    "        `label_index` method below.\n",
    "\n",
    "        :param tagged_sentences: An iterator over sentences, where each sentence\n",
    "        is a tuple of a sequence of tokens and a corresponding sequence of\n",
    "        part-of-speech tags.\n",
    "        \n",
    "        :return: A tuple of (feature-matrix, label-vector).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # break the tagged sentences into feature matrix, label vector\n",
    "        features = []\n",
    "        labels = []\n",
    "        for tokens_tags in tagged_sentences:\n",
    "            tokens, pos_tags = tokens_tags\n",
    "            # at least one feature for each token, and one for POS\n",
    "#             features.append({'token': tokens[0], 'pos-1': '<s>'})\n",
    "            features.append({'token': tokens[0], 'token-1': '<s>', 'pos-1': '<s>'})\n",
    "            labels.append(pos_tags[0])\n",
    "            \n",
    "            if len(tokens) > 1:\n",
    "                for i in range(1, len(tokens)):\n",
    "#                     features.append({'token': tokens[i], 'pos-1': pos_tags[i - 1]})\n",
    "                    features.append({'token': tokens[i], 'token-1': tokens[i - 1], 'pos-1': pos_tags[i-1]})\n",
    "                    labels.append(pos_tags[i])\n",
    "        \n",
    "#         print(\"original features: \", features)\n",
    "#         print(\"original labels: \", labels)\n",
    "#         feature_matrix and label_vector\n",
    "#         self.feature_encoder.fit(features)\n",
    "#         print(\"features length: \", len(features))\n",
    "        feature_matrix = self.feature_encoder.fit_transform(features)\n",
    "#         print(\"feature_matrix after transform\", feature_matrix)\n",
    "        \n",
    "#         print(\"label length: \", len(labels))\n",
    "#         self.label_encoder.fit(labels)\n",
    "        label_vector = self.label_encoder.fit_transform(labels)\n",
    "#         print(\"label_vector after transform\", label_vector)\n",
    "        \n",
    "        # train the model with features and labels\n",
    "        self.trained_model = self.model.fit(feature_matrix, label_vector)\n",
    "        \n",
    "        return (feature_matrix, label_vector)\n",
    "        \n",
    "#         raise NotImplementedError()\n",
    "\n",
    "    def feature_index(self, feature: Text) -> int:\n",
    "        \"\"\"\n",
    "        Returns the column index corresponding to the given named feature.\n",
    "\n",
    "        The `train` method should always be called before this method is called.\n",
    "\n",
    "        :param feature: The string name of a feature.\n",
    "        \n",
    "        :return: The column index of the feature in the feature matrix returned\n",
    "        by the `train` method.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        feature_names = self.feature_encoder.get_feature_names()\n",
    "#         print(\"feature_names returned by feature index: \", feature_names)\n",
    "        return feature_names.index(feature)\n",
    "#         raise NotImplementedError()\n",
    "\n",
    "    def label_index(self, label: Text) -> int:\n",
    "        \"\"\"\n",
    "        Returns the integer corresponding to the given part-of-speech tag\n",
    "\n",
    "        The `train` method should always be called before this method is called.\n",
    "\n",
    "        :param label: The part-of-speech tag string.\n",
    "        \n",
    "        :return: The integer for the part-of-speech tag, to be used in the label\n",
    "        vector returned by the `train` method.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return list(self.label_encoder.classes_).index(label)\n",
    "        \n",
    "#         raise NotImplementedError()\n",
    "\n",
    "    def predict(self, tokens: TokenSeq) -> TagSeq:\n",
    "        \"\"\"\n",
    "        Predicts part-of-speech tags for the sequence of tokens.\n",
    "\n",
    "        This method delegates to either `predict_greedy` or `predict_viterbi`.\n",
    "        The implementer may decide which one to delegate to.\n",
    "\n",
    "        :param tokens: A sequence of tokens representing a sentence.\n",
    "        \n",
    "        :return: A sequence of part-of-speech tags, one for each token.\n",
    "        \"\"\"\n",
    "        _, pos_tags = self.predict_greedy(tokens)\n",
    "        # _, _, pos_tags = self.predict_viterbi(tokens)\n",
    "        return pos_tags\n",
    "\n",
    "    def predict_greedy(self, tokens: TokenSeq) -> Tuple[NDArray, TagSeq]:\n",
    "        \"\"\"\n",
    "        Predicts part-of-speech tags for the sequence of tokens using a\n",
    "        greedy algorithm, and returns the feature matrix and predicted tags.\n",
    "\n",
    "        Each part-of-speech tag is predicted one at a time, and each prediction\n",
    "        is considered a hard decision, that is, when predicting the\n",
    "        part-of-speech tag for token i, the model will assume that its\n",
    "        prediction for token i-1 is correct and unchangeable.\n",
    "\n",
    "        The feature matrix should have one row per input token, and be formatted\n",
    "        in the same way as the feature matrix in `train`.\n",
    "\n",
    "        :param tokens: A sequence of tokens representing a sentence.\n",
    "        \n",
    "        :return: The feature matrix and the sequence of predicted part-of-speech\n",
    "        tags (one for each input token).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # Greedy: consider i-1\n",
    "        \n",
    "        # format the feature matrix as in 'train': one token per row, the input tokens are in one sentence\n",
    "        features = []\n",
    "        features.append({'token': tokens[0], 'token-1': '<s>'})\n",
    "        \n",
    "        if len(tokens) > 1 :\n",
    "            for i in range(1, len(tokens)):\n",
    "                features.append({'token': tokens[i], 'token-1': tokens[i - 1]})\n",
    "        \n",
    "        feature_matrix = self.feature_encoder.transform(features)\n",
    "        predicted_label_matrix = self.trained_model.predict(feature_matrix)\n",
    "        \n",
    "        predicted_labels = list(self.label_encoder.inverse_transform(predicted_label_matrix))\n",
    "        \n",
    "        p_features = []\n",
    "        p_features.append({'pos-1': '#'})\n",
    "        for labels in predicted_labels[:-1]:\n",
    "            p_features.append({'pos-1': labels})\n",
    "        \n",
    "        predicted_feature_matrix = self.feature_encoder.transform(p_features).toarray()\n",
    "        \n",
    "        return (predicted_feature_matrix, predicted_labels)\n",
    "        \n",
    "#         raise NotImplementedError()\n",
    "\n",
    "    # BONUS (not required)\n",
    "    def predict_viterbi(self, tokens: TokenSeq) -> Tuple[NDArray, NDArray, TagSeq]:\n",
    "        \"\"\"\n",
    "        Predicts part-of-speech tags for the sequence of tokens using the\n",
    "        Viterbi algorithm, and returns the transition probability tensor,\n",
    "        the Viterbi lattice, and the predicted tags.\n",
    "\n",
    "        The entry i,j,k in the transition probability tensor should correspond\n",
    "        to the log-probability estimated by the classifier of token i having\n",
    "        part-of-speech tag k, given that the previous part-of-speech tag was j.\n",
    "        Thus, the first dimension should match the number of tokens, the second\n",
    "        dimension should be one more than the number of part of speech tags (the\n",
    "        last entry in this dimension corresponds to \"<s>\"), and the third\n",
    "        dimension should match the number of part-of-speech tags.\n",
    "\n",
    "        The entry i,k in the Viterbi lattice should correspond to the maximum\n",
    "        log-probability achievable via any path from token 0 to token i and\n",
    "        ending at assigning token i the part-of-speech tag k.\n",
    "\n",
    "        The predicted part-of-speech tags should correspond to the highest\n",
    "        probability path through the lattice.\n",
    "\n",
    "        :param tokens: A sequence of tokens representing a sentence.\n",
    "        \n",
    "        :return: The transition probability tensor, the Viterbi lattice, and the\n",
    "        sequence of predicted part-of-speech tags (one for each input token).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        total_tags = list(self.label_encoder.classes_)\n",
    "        pre_tags = total_tags + [\"<s>\"]\n",
    "        \n",
    "        # i, j, k tensor\n",
    "        # i = len(tokens)\n",
    "        # j = len(total_tags) + 1 = len(pre_tags)\n",
    "        # k = len(total_tags)\n",
    "        transition_table = np.zeros((len(tokens), len(pre_tags), len(total_tags)))\n",
    "        \n",
    "        # i = len(tokens)\n",
    "        # k = len(total_tags)\n",
    "        viterbi_lattice = np.zeros((len(tokens), len(total_tags)))\n",
    "        \n",
    "        print(\"transition prob table shape: \", transition_table.shape)\n",
    "        print(\"viterbi lattice shape: \", viterbi_lattice.shape)\n",
    "        \n",
    "        # transition table grid\n",
    "        for i in range(0, len(tokens)):\n",
    "            for j in range(0, len(pre_tags)):\n",
    "                word = [{'token': tokens[i], 'pos-1': pre_tags[j]}]\n",
    "                feature_of_word = self.feature_encoder.transform(word)\n",
    "                transition_table[i][j] = self.trained_model.predict_log_proba(feature_of_word)\n",
    "        \n",
    "        # initialize base case, run viterbi\n",
    "        viterbi_lattice[0] = transition_table[0, len(total_tags)]\n",
    "        predicted_labels = [total_tags[np.argmax(viterbi_lattice[0])]]\n",
    "        for i in range(1, len(tokens)):\n",
    "            for k in range(0, len(total_tags)):\n",
    "                list_p = []\n",
    "                for j in range(0, len(total_tags)):\n",
    "                    list_p.append(viterbi_lattice[i-1][j] + transition_table[i][j][k])\n",
    "                \n",
    "                # highest probability\n",
    "                max_index = np.argmax(list_p)\n",
    "                viterbi_lattice[i][k] = list_p[max_index]\n",
    "            \n",
    "            max_pro_label = total_tags[np.argmax(viterbi_lattice[i])]\n",
    "            predicted_labels.append(max_pro_label)\n",
    "        \n",
    "        return (transition_table, viterbi_lattice, predicted_labels)\n",
    "                \n",
    "                \n",
    "#         raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb0b0eda0c3f2dd2ba77c812ca6b55c5",
     "grade": false,
     "grade_id": "ptb-tags",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# part-of-speech tags from the Penn Treebank\n",
    "PTB_TAGS = {\n",
    "    \"#\", \"$\", \"''\", \"``\", \",\", \"-LRB-\", \"-RRB-\", \".\", \":\", \"CC\", \"CD\", \"DT\",\n",
    "    \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNP\", \"NNPS\",\n",
    "    \"NNS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"SYM\", \"TO\",\n",
    "    \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d72a64679957dba3c3774e94eb42c81",
     "grade": false,
     "grade_id": "md-test-read-data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test `.read_ptbtagged()` (3 pts)\n",
    "\n",
    "Tests that you read in a) the correct number of sentences and tokens from the training data, b) that all `PTB_TAGS` were found in that partition of the data, and c) each token has exactly one corresponding tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22de0cc79b92c99147f03b35deec9a8a",
     "grade": true,
     "grade_id": "test-read-data",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_read_ptbtagged():\n",
    "    # keep a counter here (instead of enumerate) in case the iterator is empty\n",
    "    token_count = 0\n",
    "    sentence_count = 0\n",
    "    for sentence in read_ptbtagged(\"data/PTBSmall/train.tagged\"):\n",
    "        assert len(sentence) == 2\n",
    "        tokens, pos_tags = sentence\n",
    "        assert len(tokens) == len(pos_tags)\n",
    "        assert all(pos in PTB_TAGS for pos in pos_tags)\n",
    "        token_count += len(tokens)\n",
    "        sentence_count += 1\n",
    "    assert token_count == 191969\n",
    "    assert sentence_count == 8020\n",
    "\n",
    "    # check the sentence count in the dev set too\n",
    "    assert sum(1 for _ in read_ptbtagged(\"data/PTBSmall/dev.tagged\")) == 5039\n",
    "    \n",
    "test_read_ptbtagged()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fcfce5931f1fa947cfdb7a437446850",
     "grade": false,
     "grade_id": "md-test-features",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test features (5 pts)\n",
    "\n",
    "This test ensures you are, per the definition of MEMM, minimally representing **token** and **prior tag** ($t_{i-1}$) features.  \n",
    "\n",
    "Use the special symbol `<s>` to represent the prior tag of the first token in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73bb9b1a88027c59213aa14af31a2276",
     "grade": true,
     "grade_id": "test-features",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_feature_vectors():\n",
    "    clf       = Classifier()\n",
    "    ptb_train = read_ptbtagged(\"data/PTBSmall/train.tagged\")\n",
    "    ptb_train = itertools.islice(ptb_train, 2)  # just the first 2 sentences\n",
    "    features_matrix, labels_vector = clf.train(ptb_train)\n",
    "    # num. tokens\n",
    "    assert features_matrix.shape[0] == 31\n",
    "    assert labels_vector.shape[0] == 31\n",
    "\n",
    "    # train.tagged starts with\n",
    "    # Pierre\tNNP\n",
    "    # Vinken\tNNP\n",
    "    # ,\t,\n",
    "    # 61\tCD\n",
    "    # years\tNNS\n",
    "    # old\tJJ\n",
    "    assert features_matrix[4, clf.feature_index(\"token=years\")] == 1\n",
    "    assert features_matrix[4, clf.feature_index(\"token=old\")] == 0\n",
    "    assert features_matrix[4, clf.feature_index(\"pos-1=CD\")] == 1\n",
    "    assert features_matrix[4, clf.feature_index(\"pos-1=NNS\")] == 0\n",
    "    assert features_matrix[0, clf.feature_index(\"pos-1=<s>\")] == 1\n",
    "    assert labels_vector[3] == clf.label_index(\"CD\")\n",
    "    assert labels_vector[4] == clf.label_index(\"NNS\")\n",
    "test_feature_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "354c6985b109e27dd20b3dbecfaff9a0",
     "grade": false,
     "grade_id": "md-test-greedy-decoding",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test greedy decoding (5pts)\n",
    "\n",
    "In the greedy decoding approach, each tag is predicted one at a time, and each prediction is considered a **hard** decision.  In other words, when predicting the tag for token $t_{i}$, the model will assume that its prediction for the prior token $t_{i-1}$ is correct and unchangeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fd83a94de32643a135021a5b341edec",
     "grade": true,
     "grade_id": "test-greedy-decoding",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_predict_greedy():\n",
    "    clf        = Classifier()\n",
    "    ptb_train  = read_ptbtagged(\"data/PTBSmall/train.tagged\")\n",
    "    ptb_train  = itertools.islice(ptb_train, 2)  # just the 1st 2 sentences\n",
    "    clf.train(ptb_train)\n",
    "\n",
    "    tokens = \"Vinken is a director .\".split()\n",
    "    features_matrix, pos_tags = clf.predict_greedy(tokens)\n",
    "\n",
    "    # check that there is one feature vector per POS tag\n",
    "    assert features_matrix.shape[0] == len(pos_tags)\n",
    "\n",
    "    # check that all POS tags are in the PTB tagset\n",
    "    assert all(pos_tag in PTB_TAGS for pos_tag in pos_tags)\n",
    "\n",
    "    def last_pos_index(ptb_tag):\n",
    "        return clf.feature_index(\"pos-1=\" + ptb_tag)\n",
    "\n",
    "    # check that the first word (\"The\") has no pos-1 feature\n",
    "    for ptb_tag in {\"NNP\", \",\", \"CD\", \"NNS\", \"JJ\", \"MD\", \"VB\", \"DT\", \"NN\", \"IN\",\n",
    "                    \"VBZ\", \"VBG\"}:\n",
    "        assert features_matrix[0, last_pos_index(ptb_tag)] == 0\n",
    "\n",
    "    # check that the remaining words have the correct pos-1 features\n",
    "    for i, pos_tag in enumerate(pos_tags[:-1]):\n",
    "        assert features_matrix[i + 1, last_pos_index(pos_tag)] > 0\n",
    "\n",
    "test_predict_greedy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b7192194e5d8e39c77a667a8915acfd",
     "grade": false,
     "grade_id": "md-min-min-accuracy",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Minimum accuracy (4pts)\n",
    "\n",
    "Your model should achieve >= 93% acccuracy against the first 100 sentences of the Penn Treebank development partition.  To achieve this accuracy, you may need to include additional contextual features (i.e., features that represent information about the surrounding words and/or tags).\n",
    "\n",
    "**WARNING**: _this test may be slow to run (2 min.+)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cd5cab7ce4d9ceca31116153f7dbd69",
     "grade": true,
     "grade_id": "test-min-accuracy",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "94.2% accuracy on first 100 sentences of PTB dev\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy():\n",
    "    clf       = Classifier()\n",
    "    ptb_train = read_ptbtagged(\"data/PTBSmall/train.tagged\")\n",
    "    clf.train(ptb_train)\n",
    "\n",
    "    total_count   = 0\n",
    "    correct_count = 0\n",
    "    ptb_dev = read_ptbtagged(\"data/PTBSmall/dev.tagged\")\n",
    "    ptb_dev = itertools.islice(ptb_dev, 100)  # just the 1st 100 sentences\n",
    "    for tokens, pos_tags in ptb_dev:\n",
    "        total_count += len(tokens)\n",
    "        predicted_tags = clf.predict(tokens)\n",
    "        assert len(predicted_tags) == len(pos_tags)\n",
    "        for predicted_tag, true_tag in zip(predicted_tags, pos_tags):\n",
    "            if predicted_tag == true_tag:\n",
    "                correct_count += 1\n",
    "    accuracy = correct_count / total_count\n",
    "\n",
    "    # print out performance\n",
    "    sg = f\"\\n{accuracy:.1%} accuracy on first 100 sentences of PTB dev\"\n",
    "    print(sg)\n",
    "    \n",
    "    assert accuracy >= 0.93\n",
    "\n",
    "test_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS TEST (+5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition prob table shape:  (5, 14, 13)\n",
      "viterbi lattice shape:  (5, 13)\n"
     ]
    }
   ],
   "source": [
    "def test_predict_viterbi():\n",
    "    clf        = Classifier()\n",
    "    ptb_train  = read_ptbtagged(\"data/PTBSmall/train.tagged\")\n",
    "    ptb_train  = itertools.islice(ptb_train, 2)  # just the 1st 2 sentences\n",
    "    clf.train(ptb_train)\n",
    "\n",
    "    # POS tags in first 2 sentences\n",
    "    possible_tags = {\"NNP\", \",\", \"CD\", \"NNS\", \"JJ\", \"MD\", \"VB\", \"DT\", \"NN\",\n",
    "                     \"IN\", \".\", \"VBZ\", \"VBG\"}\n",
    "    n_tags = len(possible_tags)\n",
    "\n",
    "    # sample sentence to be fed to classifier\n",
    "    tokens = \"Vinken is a director .\".split()\n",
    "    trans_probs, viterbi_lattice, pos_tags = clf.predict_viterbi(tokens)\n",
    "\n",
    "    # check that the transition probabilities are the right shape\n",
    "    # second axis is +1 since the last entry is transitions starting at <s>\n",
    "    assert trans_probs.shape == (len(tokens), n_tags + 1, n_tags)\n",
    "\n",
    "    # check that probability distribution from <s> to first word sums to 1\n",
    "    s_index = n_tags\n",
    "    prob_dist_sums = np.sum(np.exp(trans_probs), axis=-1)\n",
    "    np.testing.assert_almost_equal(prob_dist_sums[0, s_index], 1)\n",
    "\n",
    "    # check that probability distributions of all non-<s> pairs sum to 1\n",
    "    for i in range(1, len(tokens)):\n",
    "        for j in range(0, n_tags):\n",
    "            np.testing.assert_almost_equal(prob_dist_sums[i, j], 1)\n",
    "\n",
    "    # check that the lattice is the right shape\n",
    "    assert viterbi_lattice.shape == (len(tokens), n_tags)\n",
    "\n",
    "    # check that the numbers are not all the same in the lattice\n",
    "    assert np.std(viterbi_lattice) > 0\n",
    "    assert np.all(np.std(viterbi_lattice, axis=0) > 0)\n",
    "    assert np.all(np.std(viterbi_lattice, axis=1) > 0)\n",
    "\n",
    "    # check that the probabilities from <s> are on the first token\n",
    "    np.testing.assert_almost_equal(viterbi_lattice[0], trans_probs[0, s_index])\n",
    "\n",
    "    # check some probability calculations in the lattice\n",
    "    np.testing.assert_almost_equal(viterbi_lattice[1, 2], max([\n",
    "        viterbi_lattice[0, k] + trans_probs[1, k, 2] for k in range(n_tags)]))\n",
    "    np.testing.assert_almost_equal(viterbi_lattice[3, 1], max([\n",
    "        viterbi_lattice[2, k] + trans_probs[3, k, 1] for k in range(n_tags)]))\n",
    "    np.testing.assert_almost_equal(viterbi_lattice[-1, 9], max([\n",
    "        viterbi_lattice[-2, k] + trans_probs[-1, k, 9] for k in range(n_tags)]))\n",
    "\n",
    "    # check that the POS tags are all valid tags\n",
    "    assert all([pos_tag in PTB_TAGS for pos_tag in pos_tags])\n",
    "\n",
    "    # check that the lattice's score for the predicted POS tag path matches\n",
    "    # the score we would get from the transition probabilities\n",
    "    pos_indexes = [clf.label_index(t) for t in pos_tags]\n",
    "    np.testing.assert_almost_equal(\n",
    "        viterbi_lattice[-1, pos_indexes[-1]],\n",
    "        sum(trans_probs[i, pos_indexes[i - 1] if i else s_index, pos_index]\n",
    "            for i, pos_index in enumerate(pos_indexes)))\n",
    "\n",
    "    # check that the selected POS path has the highest score of all the possible\n",
    "    # paths through the lattice\n",
    "    np.testing.assert_almost_equal(\n",
    "        viterbi_lattice[-1, pos_indexes[-1]],\n",
    "        max(trans_probs[0, s_index, index1] +\n",
    "            trans_probs[1, index1, index2] +\n",
    "            trans_probs[2, index2, index3] +\n",
    "            trans_probs[3, index3, index4] +\n",
    "            trans_probs[4, index4, index5]\n",
    "            for index1 in range(n_tags)\n",
    "            for index2 in range(n_tags)\n",
    "            for index3 in range(n_tags)\n",
    "            for index4 in range(n_tags)\n",
    "            for index5 in range(n_tags)))\n",
    "\n",
    "test_predict_viterbi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
